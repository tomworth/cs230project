{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "siamesepaintingcomp.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pm3AktNbLmoe"
      },
      "source": [
        "##This notebook contains all CNN architectures for the Siamese comparison, as well as the K-Nearest-Neighbors code since that just uses the pre-trained SCNN.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWHESq9kWeLW"
      },
      "source": [
        "!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "!pip install tensorflow_hub\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "import datetime\n",
        "import requests\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "np.set_printoptions(precision=4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2468gGReAC7"
      },
      "source": [
        "### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATGE_5YEWz7j",
        "outputId": "0a112cdc-b443-476f-c35a-0b95a748789c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\"\"\"\n",
        "!pip install google.colab\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KyJkpr_VSE98"
      },
      "source": [
        "Create a dictionary in which keys are all the artists and values are all of the images by that artist:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNa1oGmweW9p",
        "outputId": "29f31291-3d85-4f5d-c446-9fa3a95607cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "artistdict = {}\n",
        "\n",
        "with open('/content/drive/My Drive/train_info.csv', 'r') as csvfile:\n",
        "  table = csv.reader(csvfile)\n",
        "  c = 0\n",
        "  for row in table:\n",
        "    if c == 0:\n",
        "      c += 1\n",
        "      continue\n",
        "    if row[1] not in artistdict:\n",
        "      artistdict[row[1]] = []\n",
        "    artistdict[row[1]].append(row[0])\n",
        "    c += 1\n",
        "    \n",
        "print(c)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79434\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0t1bk8dSXR-"
      },
      "source": [
        "Create a balanced set of labeled image pairs:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5OTVCw6AlIj",
        "outputId": "b5921c84-2e55-4bca-ba91-0fc48e307e47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "datapoints = []\n",
        "badcount=0\n",
        "for artist in artistdict.keys():\n",
        "  paintings = artistdict[artist]\n",
        "  if len(paintings) < 31:\n",
        "    continue\n",
        "  for i in range(30):\n",
        "    left = paintings[i]\n",
        "    if left[0] != '9':\n",
        "      continue\n",
        "    for j in range(i+1, 30):\n",
        "      random.seed(j)\n",
        "      right = paintings[j]\n",
        "      if right[0] != '9':\n",
        "        continue\n",
        "      datapoints.append([left, right, 1])\n",
        "      if len(datapoints) <= 60:\n",
        "        k = j\n",
        "        while True:\n",
        "          otherartist = random.choice(list(artistdict))\n",
        "          if otherartist == artist:\n",
        "            k += 1\n",
        "            random.seed(k)\n",
        "            continue\n",
        "          otherpainting = random.choice(artistdict[otherartist])\n",
        "          if otherpainting[0] != '9':\n",
        "            continue\n",
        "          break\n",
        "      else:\n",
        "        otherpainting = random.choice(datapoints[:-30])\n",
        "        otherpainting = otherpainting[0]\n",
        "        \n",
        "      datapoints.append([left, otherpainting, 0])\n",
        "\n",
        "random.shuffle(datapoints)\n",
        "\n",
        "print(len(datapoints))\n",
        "print(badcount)\n",
        "\n",
        "uniquepaintings = []\n",
        "\n",
        "for datapoint in datapoints:\n",
        "  if datapoint[0] not in uniquepaintings:\n",
        "    uniquepaintings.append(datapoint[0])\n",
        "  if datapoint[1] not in uniquepaintings:\n",
        "    uniquepaintings.append(datapoint[1])\n",
        "print(len(uniquepaintings))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6708\n",
            "0\n",
            "2089\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQKVNGl-q_AW"
      },
      "source": [
        "###Creating Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7UbVT2DSrmZg",
        "outputId": "ed26f6a2-f4fb-4d79-fbe9-be59b03b4409",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "!pip install pillow\n",
        "from PIL import Image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pillow in ./anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages (7.0.0)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRGHrVmVoX3L"
      },
      "source": [
        "fails = 0\n",
        "count = 0\n",
        "for painting in uniquepaintings:\n",
        "  imagename = \"/content/drive/My Drive/kaggle/train_9/\" + painting\n",
        "  destname = '/content/train9unique/' + painting\n",
        "  try:\n",
        "    image = Image.open(imagename).convert('RGB').resize((224, 224))\n",
        "  except:\n",
        "    fails += 1\n",
        "  image.save(destname, 'JPEG')\n",
        "  if count % 50 == 0:\n",
        "    print(count)\n",
        "  count += 1\n",
        "print(fails)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIDYwfIGSd0L"
      },
      "source": [
        "Process the datapoints into numpy array format:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lO7q0fGKmQ1"
      },
      "source": [
        "from numpy import asarray"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRG0dNuHQKKV"
      },
      "source": [
        "def names_to_images(datapoint):\n",
        "  leftimage = datapoint[0]\n",
        "  rightimage = datapoint[1]\n",
        "  label = datapoint[2]\n",
        "\n",
        "  leftfilename = '/content/drive/My Drive/kaggle/train_9/' + leftimage\n",
        "  rightfilename = '/content/drive/My Drive/kaggle/train_9/' + rightimage\n",
        "\n",
        "  \n",
        "  leftimage = Image.open(leftfilename).resize((224, 224))\n",
        "  leftimage = asarray(leftimage)\n",
        "  \n",
        "  rightimage = Image.open(rightfilename).resize((224, 224))\n",
        "  rightimage = asarray(rightimage)\n",
        "  \n",
        "\n",
        "  assert(leftimage.shape[2] == 3 and rightimage.shape[2] == 3)\n",
        "\n",
        "  return leftimage, rightimage, label\n",
        "\n",
        "m = len(datapoints)\n",
        "#m = 1400 #for Colab run\n",
        "\n",
        "\n",
        "i=0\n",
        "print(len(datapoints))\n",
        "\n",
        "dataset_size = m\n",
        "\n",
        "leftimages = np.empty((dataset_size, 224, 224, 3))\n",
        "rightimages = np.empty((dataset_size, 224, 224, 3))\n",
        "labels = np.empty((dataset_size))\n",
        "\n",
        "fail = 0\n",
        "\n",
        "for datapoint in datapoints:\n",
        "  try:\n",
        "    leftimage, rightimage, label = names_to_images(datapoint)\n",
        "  except:\n",
        "    fail += 1\n",
        "    pass\n",
        "\n",
        "  leftimages[i,:,:,:] = leftimage[:,:,:] \n",
        "  rightimages[i,:,:,:] = rightimage[:,:,:]\n",
        "\n",
        "  labels[i] = label\n",
        "  i+=1\n",
        "  if i == dataset_size:\n",
        "    break\n",
        "  if i % 50 == 0:\n",
        "    print(i)\n",
        "\n",
        "\n",
        "print(fail)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfWq6v1QSkty"
      },
      "source": [
        "Define train, val, and test sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoqIl1VAInFX",
        "outputId": "eb11ca28-64a2-4359-ed4f-d52d4e71b42a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "train_size = int(0.7 * dataset_size)\n",
        "val_size = int(0.15 * dataset_size)\n",
        "test_size = int(0.15 * dataset_size)\n",
        "\n",
        "\n",
        "leftimg_train = leftimages[:train_size,:,:,:]\n",
        "leftimg_val = leftimages[train_size:train_size + val_size,:,:,:]\n",
        "leftimg_test = leftimages[train_size + val_size:,:,:,:]\n",
        "\n",
        "rightimg_train = rightimages[:train_size,:,:,:]\n",
        "rightimg_val = rightimages[train_size:train_size + val_size,:,:,:]\n",
        "rightimg_test = rightimages[train_size + val_size:,:,:,:]\n",
        "\n",
        "label_train = labels[:train_size]\n",
        "label_val = labels[train_size:train_size + val_size]\n",
        "label_test = labels[train_size + val_size:]\n",
        "\n",
        "\n",
        "print(leftimages.shape)\n",
        "print(leftimg_train.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1400, 224, 224, 3)\n",
            "(979, 224, 224, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DZ___aNeLYs"
      },
      "source": [
        "# Creating Model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypzAnQfmQL5X"
      },
      "source": [
        "(Define inputs)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpuiGw6QP7C7"
      },
      "source": [
        "input_shape = [224, 224, 3]\n",
        "left_input = tf.keras.Input(input_shape)\n",
        "right_input = tf.keras.Input(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrS1oQkwkliQ"
      },
      "source": [
        "Model -1. Multilayer Perceptron"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hWlg-T3kn_G"
      },
      "source": [
        "cnn = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(1200, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STfSpS_Xp4a0"
      },
      "source": [
        "Model 0. Vanilla CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRDzhwRQp4EN"
      },
      "source": [
        "cnn = tf.keras.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(32, kernel_size=(5, 5), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dense(128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSI5IoH1Yivt"
      },
      "source": [
        "Model 0.5. Bigger Vanilla"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBOFInmQYl1I"
      },
      "source": [
        "cnn = tf.keras.Sequential()\n",
        "cnn.add(tf.keras.layers.Conv2D(8, kernel_size=(5, 5), strides=(1, 1),\n",
        "                 activation='relu',\n",
        "                 input_shape=input_shape))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Conv2D(16, (5, 5), activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Conv2D(32, (5, 5), activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Conv2D(64, (5, 5), activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Conv2D(128, (5, 5), activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(tf.keras.layers.Flatten())\n",
        "cnn.add(tf.keras.layers.Dense(1000, activation='relu'))\n",
        "cnn.add(tf.keras.layers.Dense(128, activation='relu'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIv7ZNuDPf9z"
      },
      "source": [
        "Model 1. Resnet-50, NOT trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RC-xvcgaPcW"
      },
      "source": [
        "\n",
        "\n",
        "cnn = tf.keras.Sequential([\n",
        "                             hub.KerasLayer(\"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\", trainable=False),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "]) # inputs size (None, 224, 224, 3) ; outputs size (128)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swMpi7aDPlLd"
      },
      "source": [
        "Model 2. Resnet-50, Trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XBOFUo0PotT"
      },
      "source": [
        "\n",
        "\n",
        "cnn = tf.keras.Sequential([\n",
        "                             hub.KerasLayer(\"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\", trainable=True),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "]) # inputs size (None, 224, 224, 3) ; outputs size (128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiZ75aX_Qjse"
      },
      "source": [
        "Model 3. Inception V3, NOT trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZdHwrDjQjF0"
      },
      "source": [
        "\n",
        "cnn = tf.keras.Sequential([\n",
        "                             hub.KerasLayer(\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/4\", trainable=False),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "]) # inputs size (None, 224, 224, 3) ; outputs size (128)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV2kUIACfBxj"
      },
      "source": [
        "Model 4. EfficientNet, NOT trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eZ-XfQnfF7m"
      },
      "source": [
        "cnn = tf.keras.Sequential([\n",
        "                             hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\", trainable=False),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "]) # inputs size (None, 224, 224, 3) ; outputs size (128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGIGN7qBkCbx"
      },
      "source": [
        "Model 5. EfficientNet, Trainable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NrdutPh5kFd0"
      },
      "source": [
        "cnn = tf.keras.Sequential([\n",
        "                             hub.KerasLayer(\"https://tfhub.dev/tensorflow/efficientnet/b7/feature-vector/1\", trainable=True),\n",
        "                             tf.keras.layers.Dense(800, activation=\"relu\", trainable=True),\n",
        "                             tf.keras.layers.Dense(128, activation=\"relu\", trainable=True)\n",
        "\n",
        "]) # inputs size (None, 224, 224, 3) ; outputs size (128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMdphNa5QIrB"
      },
      "source": [
        "(The rest of the model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQlNR8GQQFFs"
      },
      "source": [
        "encoded_l = cnn(left_input)\n",
        "encoded_r = cnn(right_input)\n",
        "\n",
        "L1_layer = tf.keras.layers.Lambda(lambda tensors:tf.math.abs(tensors[0] - tensors[1]))\n",
        "L1_distance = L1_layer([encoded_l, encoded_r])\n",
        "\n",
        "prediction = tf.keras.layers.Dense(1, activation=\"sigmoid\", trainable=True)(L1_distance)\n",
        "siamese_net = tf.keras.Model(inputs=[left_input, right_input], outputs=prediction)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDEOOAEpSqIv"
      },
      "source": [
        "Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyUgeOswd8fX"
      },
      "source": [
        "siamese_net.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer='adam')\n",
        "siamese_net.summary()\n",
        "\n",
        "history = siamese_net.fit([leftimg_train, rightimg_train], label_train, epochs=8, steps_per_epoch = 150)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc2fHNsXSsal"
      },
      "source": [
        "Evaluating the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLFd3vQ96vPT",
        "outputId": "1d82d5ba-4fa0-470a-d73c-3bf7211bf00d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "siamese_net.evaluate([leftimg_val, rightimg_val], label_val)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 13s 2s/step - loss: 0.9330 - acc: 0.5476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.9329984188079834, 0.5476190447807312]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8AkrUjRNBFt"
      },
      "source": [
        "Fine-tuning the transfer learning models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIJzVw9vvIAM"
      },
      "source": [
        "cnn.trainable = True\n",
        "siamese_net.compile(loss=\"binary_crossentropy\", metrics=['acc'], optimizer=keras.optimizers.Adam(1e-5))\n",
        "siamese_net.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3bLWwwO_Ss2s"
      },
      "source": [
        "history = siamese_net.fit([leftimg_train, rightimg_train], label_train, epochs=4, steps_per_epoch = 120)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBWW4BERb1Xu"
      },
      "source": [
        "siamese_net.evaluate([leftimg_val, rightimg_val], label_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ITgy5pA5fM_"
      },
      "source": [
        "##Price Prediction: K-Nearest-Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPgUeRHwceHP"
      },
      "source": [
        "with open('phillips2.csv') as f:\n",
        "    lots = [{k: v for k, v in row.items()}\n",
        "        for row in csv.DictReader(f, skipinitialspace=True)]\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3XaraI6ne9",
        "outputId": "0f8b3a47-e760-4eb2-9d9f-e1a08414dbbc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "fails = 0\n",
        "for lot in lots:\n",
        "  try:\n",
        "    int(lot['day'])\n",
        "    int(lot['month'])\n",
        "    int(lot['year'])\n",
        "  except:\n",
        "    print(lot)\n",
        "    lots.remove(lot)\n",
        "    fails += 1\n",
        "print(fails)\n",
        "print(len(lots))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "23825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ilZi4hUTDuq"
      },
      "source": [
        "###K-Nearest-Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJdG0ACY8p5B"
      },
      "source": [
        "  lots = sorted(lots, key=lambda lot: int(lot['day']))\n",
        "  lots = sorted(lots, key=lambda lot: int(lot['month']))\n",
        "  lots = sorted(lots, key=lambda lot: int(lot['year']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHoGnOqVxHDQ"
      },
      "source": [
        "def knearest(k, n, lots):\n",
        "  #MIGHT RETURN NONE!\n",
        "\n",
        "  idx = random.randint(n+100, len(lots))\n",
        "  topredict = lots[idx]\n",
        "\n",
        "  i = 1\n",
        "  while True:\n",
        "    last_before_auction_idx = idx - i\n",
        "    last_before_auction = lots[last_before_auction_idx]\n",
        "    lastn = lots[last_before_auction_idx-n : last_before_auction_idx+1]\n",
        "    if last_before_auction['day'] != topredict['day'] or last_before_auction['month'] != topredict['month']:\n",
        "      break\n",
        "    i += 1\n",
        "\n",
        "  priceandprob = []\n",
        "\n",
        "  inputtopred = np.zeros((1, 224, 224, 3))\n",
        "  try:\n",
        "    predimg = tf.image.decode_jpeg(requests.get(topredict[\"imageurl\"]).content, channels=3)\n",
        "    lastimage = predimg\n",
        "  except:\n",
        "    return None\n",
        "  predimg = tf.image.convert_image_dtype(predimg, tf.float32)\n",
        "  predimg = tf.image.resize(predimg, [224, 224])\n",
        "  inputtopred[0,:,:,:] = predimg[:,:,:]\n",
        "\n",
        "  \n",
        "  for lot in lastn:\n",
        "\n",
        "    inputlotimg = np.zeros((1, 224, 224, 3))\n",
        "    try:\n",
        "      lotimg = tf.image.decode_jpeg(requests.get(lot[\"imageurl\"]).content, channels=3)\n",
        "      lastimage = lotimg\n",
        "    except:\n",
        "      lotimg = lastimage\n",
        "    lotimg = tf.image.convert_image_dtype(lotimg, tf.float32)\n",
        "    lotimg = tf.image.resize(lotimg, [224, 224])   \n",
        "    inputlotimg[0,:,:,:] = predimg[:,:,:]\n",
        "\n",
        "    prob = siamese_net.predict([inputlotimg, inputtopred])\n",
        "    lot['prob'] = prob\n",
        "    priceandprob.append(lot)\n",
        "\n",
        "  priceandprob = sorted(priceandprob, key=lambda lot: lot['prob'], reverse=True)\n",
        "  #sum = 0\n",
        "  #for i in range(k):\n",
        "  #  sum += float(priceandprob[i]['price'])\n",
        "  #avg = sum / k\n",
        "  \n",
        "  errors = {}\n",
        "  medians = {}\n",
        "  for i in k:\n",
        "    topi = priceandprob[:i]\n",
        "    topi = sorted(topi, key = lambda lot: lot['price'])\n",
        "    median = float(topi[i//2]['price'])\n",
        "    medians[str(i)] = median\n",
        "    mape = 100 * abs(float(topredict['price']) - median) / float(topredict['price'])\n",
        "    errors[str(i)] = mape\n",
        "\n",
        "  print('to predict: '+ topredict['price'])\n",
        "  print('prediction: '+ str(medians['70']))\n",
        "  return errors\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I3RsEz5hmovY"
      },
      "source": [
        "Messing around with K Nearest: try with very big n, smallish k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be_vjBD-ZSvl"
      },
      "source": [
        "m =  #number of attempts to average over\n",
        "sums = {}\n",
        "for i in [10, 30, 50, 70, 90]:\n",
        "  sums[str(i)] = 0\n",
        "for i in range(m):\n",
        "  errors = knearest([10, 30, 50, 70, 90], 400, lots)\n",
        "  if errors != None:\n",
        "    print(errors)\n",
        "    print(i)\n",
        "    for i in errors.keys():\n",
        "      sums[i] += errors[i]\n",
        "  else: \n",
        "    m -= 1\n",
        "\n",
        "avgerrors = {}\n",
        "for i in [10, 30, 50, 70, 90]:\n",
        "  avgerrors[str(i)] = sums[str(i)] / m\n",
        "print(avgerrors)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}